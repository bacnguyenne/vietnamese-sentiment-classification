{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9640489,"sourceType":"datasetVersion","datasetId":5885637}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install vncorenlp\n# !git clone https://github.com/vncorenlp/VnCoreNLP.git","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:47:23.877992Z","iopub.execute_input":"2024-10-17T03:47:23.878752Z","iopub.status.idle":"2024-10-17T03:47:23.882381Z","shell.execute_reply.started":"2024-10-17T03:47:23.878708Z","shell.execute_reply":"2024-10-17T03:47:23.881484Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# !wget https://github.com/vncorenlp/VnCoreNLP/archive/refs/tags/v1.2.zip\n# !unzip v1.2.zip\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:47:23.895852Z","iopub.execute_input":"2024-10-17T03:47:23.896144Z","iopub.status.idle":"2024-10-17T03:47:23.900269Z","shell.execute_reply.started":"2024-10-17T03:47:23.896112Z","shell.execute_reply":"2024-10-17T03:47:23.899390Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# from vncorenlp import VnCoreNLP\n# import os\n\n# # Khởi động VnCoreNLP\n# rdrsegmenter = VnCoreNLP(os.path.join(\"/kaggle/working/VnCoreNLP-1.2/VnCoreNLP-1.2.jar\"), annotators=\"wseg,pos,ner,parse\", max_heap_size='-Xmx2g')\n\n# # Hàm tách từ sử dụng VnCoreNLP\n# def word_segment(text):\n#     return ' '.join(rdrsegmenter.tokenize(text)[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:47:23.902402Z","iopub.execute_input":"2024-10-17T03:47:23.902780Z","iopub.status.idle":"2024-10-17T03:47:23.909646Z","shell.execute_reply.started":"2024-10-17T03:47:23.902730Z","shell.execute_reply":"2024-10-17T03:47:23.908735Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nimport pandas as pd\nimport os\nfrom tqdm.auto import tqdm\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T07:28:13.556577Z","iopub.execute_input":"2024-10-17T07:28:13.557412Z","iopub.status.idle":"2024-10-17T07:28:13.563005Z","shell.execute_reply.started":"2024-10-17T07:28:13.557372Z","shell.execute_reply":"2024-10-17T07:28:13.562070Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/data-dl-tk2/df_final_a.csv')\ndata = data.dropna().reset_index()\n# data['cleaned_segmented'] = data['cleaned_old_data'].progress_apply(word_segment)\n\n# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n# X_train, X_test, y_train, y_test = train_test_split(data['cleaned_segmented'], data['label'], test_size=0.2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(data['cleaned_old_data'], data['label'], test_size=0.2, random_state=42)\n\n# Chuyển đổi văn bản thành vector TF-IDF\ntfidf = TfidfVectorizer(max_features=5000)\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:47:23.910761Z","iopub.execute_input":"2024-10-17T03:47:23.911061Z","iopub.status.idle":"2024-10-17T03:47:27.206136Z","shell.execute_reply.started":"2024-10-17T03:47:23.911030Z","shell.execute_reply":"2024-10-17T03:47:27.205144Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Huấn luyện mô hình Naive Bayes\nmodel = MultinomialNB()\nmodel.fit(X_train_tfidf, y_train)\n\n# Dự đoán trên tập kiểm tra\ny_pred = model.predict(X_test_tfidf)\n\n# Đánh giá mô hình\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:47:27.207701Z","iopub.execute_input":"2024-10-17T03:47:27.208083Z","iopub.status.idle":"2024-10-17T03:47:27.251623Z","shell.execute_reply.started":"2024-10-17T03:47:27.208039Z","shell.execute_reply":"2024-10-17T03:47:27.250637Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.62      0.69      0.65      2380\n           1       0.55      0.47      0.51      2315\n           2       0.54      0.53      0.54      2415\n           3       0.66      0.55      0.60      2416\n           4       0.60      0.64      0.62      2257\n           5       0.65      0.75      0.69      2442\n\n    accuracy                           0.60     14225\n   macro avg       0.60      0.60      0.60     14225\nweighted avg       0.60      0.60      0.60     14225\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nclf = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial')\nclf.fit(X_train_tfidf, y_train)\n\n# Dự đoán và đánh giá mô hình\ny_pred = clf.predict(X_test_tfidf)\nprint(classification_report(y_test, y_pred, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:47:27.254604Z","iopub.execute_input":"2024-10-17T03:47:27.254989Z","iopub.status.idle":"2024-10-17T03:47:34.609542Z","shell.execute_reply.started":"2024-10-17T03:47:27.254952Z","shell.execute_reply":"2024-10-17T03:47:34.608564Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.7476    0.7378    0.7427      2380\n           1     0.6924    0.6156    0.6517      2315\n           2     0.6584    0.6513    0.6549      2415\n           3     0.7890    0.8108    0.7998      2416\n           4     0.7125    0.7523    0.7319      2257\n           5     0.8736    0.9169    0.8947      2442\n\n    accuracy                         0.7487     14225\n   macro avg     0.7456    0.7475    0.7459     14225\nweighted avg     0.7466    0.7487    0.7470     14225\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# pip install xgboost\n\nimport xgboost as xgb\n\n# Chuyển đổi nhãn thành mảng numpy\ny_train_np = y_train.values\ny_test_np = y_test.values\n\n# Tạo DMatrix cho XGBoost\ndtrain = xgb.DMatrix(X_train_tfidf, label=y_train_np)\ndtest = xgb.DMatrix(X_test_tfidf, label=y_test_np)\n\n# Thiết lập tham số cho XGBoost\nparams = {\n    'objective': 'multi:softmax',\n    'num_class': 6,\n    'eval_metric': 'merror',\n    'max_depth': 6,\n    'eta': 0.1,\n    'seed': 42\n}\n\n# Huấn luyện mô hình\nbst = xgb.train(params, dtrain, num_boost_round=100)\n\n# Dự đoán\ny_pred = bst.predict(dtest)\n\n# Đánh giá mô hình\nprint(classification_report(y_test, y_pred, digits=4))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:47:34.610744Z","iopub.execute_input":"2024-10-17T03:47:34.611060Z","iopub.status.idle":"2024-10-17T03:48:46.973412Z","shell.execute_reply.started":"2024-10-17T03:47:34.611026Z","shell.execute_reply":"2024-10-17T03:48:46.972509Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.8052    0.7971    0.8011      2380\n           1     0.7117    0.6281    0.6673      2315\n           2     0.7373    0.7251    0.7311      2415\n           3     0.8899    0.8998    0.8948      2416\n           4     0.7690    0.8631    0.8134      2257\n           5     0.9628    0.9758    0.9693      2442\n\n    accuracy                         0.8160     14225\n   macro avg     0.8127    0.8148    0.8128     14225\nweighted avg     0.8142    0.8160    0.8142     14225\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cài đặt các thư viện cần thiết\n# pip install transformers torch scikit-learn pandas\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport pandas as pd\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom torch.nn.utils import clip_grad_norm_\n\n# Thiết lập thiết bị\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Sử dụng thiết bị: {device}\")\n\n# Tải tokenizer và model PhoBERT\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\nmodel = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=6)\nmodel.to(device)\n\n# Tiền xử lý dữ liệu cho PhoBERT\ndef encode_texts(texts, tokenizer, max_length=256):\n    return tokenizer(\n        texts.tolist(),\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n\nprint(\"Bắt đầu mã hóa dữ liệu...\")\nX_train_enc = encode_texts(X_train, tokenizer)\nX_test_enc = encode_texts(X_test, tokenizer)\nprint(\"Hoàn tất mã hóa dữ liệu.\")\n\n# Chuyển đổi nhãn thành tensor\ny_train_tensor = torch.tensor(y_train.values)\ny_test_tensor = torch.tensor(y_test.values)\n\n# Tạo DataLoader\nbatch_size = 64  # Giảm batch size để tránh vấn đề về bộ nhớ GPU\n\ntrain_dataset = TensorDataset(X_train_enc['input_ids'], X_train_enc['attention_mask'], y_train_tensor)\ntest_dataset = TensorDataset(X_test_enc['input_ids'], X_test_enc['attention_mask'], y_test_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# Huấn luyện mô hình\noptimizer = AdamW(model.parameters(), lr=1e-5)  # Giảm learning rate từ 2e-5 xuống 1e-5\n\nnum_epochs = 10  # Tăng số epoch để mô hình có thời gian học nhiều hơn\ntotal_steps = len(train_loader) * num_epochs\n\n# Thiết lập scheduler để điều chỉnh learning rate\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n# Hàm tính độ chính xác\ndef compute_accuracy(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(preds_flat == labels_flat) / len(labels_flat)\n\n# Early Stopping parameters\npatience = 2\nbest_accuracy = 0\nepochs_no_improve = 0\n\nprint(\"Bắt đầu huấn luyện mô hình...\")\nfor epoch in range(num_epochs):\n    print(f\"\\n===== Epoch {epoch + 1} / {num_epochs} =====\")\n    model.train()\n    total_loss = 0\n    total_accuracy = 0\n\n    for step, batch in enumerate(train_loader):\n        model.zero_grad()\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        total_loss += loss.item()\n\n        # Tính độ chính xác trên batch hiện tại\n        logits = logits.detach().cpu().numpy()\n        label_ids = labels.cpu().numpy()\n        batch_accuracy = compute_accuracy(logits, label_ids)\n        total_accuracy += batch_accuracy\n\n        loss.backward()\n        # Clip gradients để tránh gradient explosion\n        clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n\n        if step % 10 == 0 and step != 0:\n            avg_loss = total_loss / (step + 1)\n            avg_accuracy = total_accuracy / (step + 1)\n            print(f\"  Batch {step}/{len(train_loader)} - Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n\n    avg_epoch_loss = total_loss / len(train_loader)\n    avg_epoch_accuracy = total_accuracy / len(train_loader)\n    print(f\"\\nKết quả Epoch {epoch + 1}:\")\n    print(f\"  Loss trung bình: {avg_epoch_loss:.4f}\")\n    print(f\"  Độ chính xác trung bình: {avg_epoch_accuracy:.4f}\")\n\n    # Đánh giá và kiểm tra Early Stopping\n    if avg_epoch_accuracy > best_accuracy:\n        best_accuracy = avg_epoch_accuracy\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), 'best_model.pt')\n        print(\"  -> Cập nhật mô hình tốt nhất.\")\n    else:\n        epochs_no_improve += 1\n        print(f\"  -> Không cải thiện được trong {epochs_no_improve} epoch.\")\n        if epochs_no_improve >= patience:\n            print(\"Early stopping được kích hoạt.\")\n            break\n\nprint(\"Hoàn tất huấn luyện.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:48:46.975146Z","iopub.execute_input":"2024-10-17T03:48:46.975559Z","iopub.status.idle":"2024-10-17T07:25:21.965963Z","shell.execute_reply.started":"2024-10-17T03:48:46.975505Z","shell.execute_reply":"2024-10-17T07:25:21.964204Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Sử dụng thiết bị: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8d68fda64f547799764d2a83dc1ee30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69ad4eec30174349bafa2edc9c5ae3db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2810799c7ef84469a4a8c490901e027e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e74d870ef592485c896099105018b7f4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"435784e09e9044cba25005093005071a"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Bắt đầu mã hóa dữ liệu...\nHoàn tất mã hóa dữ liệu.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Bắt đầu huấn luyện mô hình...\n\n===== Epoch 1 / 10 =====\n  Batch 10/889 - Loss: 1.7887, Accuracy: 0.1832\n  Batch 20/889 - Loss: 1.7748, Accuracy: 0.2106\n  Batch 30/889 - Loss: 1.7543, Accuracy: 0.2450\n  Batch 40/889 - Loss: 1.7401, Accuracy: 0.2645\n  Batch 50/889 - Loss: 1.7172, Accuracy: 0.2953\n  Batch 60/889 - Loss: 1.6943, Accuracy: 0.3145\n  Batch 70/889 - Loss: 1.6716, Accuracy: 0.3334\n  Batch 80/889 - Loss: 1.6501, Accuracy: 0.3492\n  Batch 90/889 - Loss: 1.6249, Accuracy: 0.3656\n  Batch 100/889 - Loss: 1.6007, Accuracy: 0.3793\n  Batch 110/889 - Loss: 1.5796, Accuracy: 0.3877\n  Batch 120/889 - Loss: 1.5590, Accuracy: 0.3972\n  Batch 130/889 - Loss: 1.5396, Accuracy: 0.4061\n  Batch 140/889 - Loss: 1.5215, Accuracy: 0.4145\n  Batch 150/889 - Loss: 1.5020, Accuracy: 0.4240\n  Batch 160/889 - Loss: 1.4868, Accuracy: 0.4299\n  Batch 170/889 - Loss: 1.4706, Accuracy: 0.4381\n  Batch 180/889 - Loss: 1.4538, Accuracy: 0.4455\n  Batch 190/889 - Loss: 1.4401, Accuracy: 0.4515\n  Batch 200/889 - Loss: 1.4265, Accuracy: 0.4562\n  Batch 210/889 - Loss: 1.4162, Accuracy: 0.4602\n  Batch 220/889 - Loss: 1.4043, Accuracy: 0.4658\n  Batch 230/889 - Loss: 1.3897, Accuracy: 0.4714\n  Batch 240/889 - Loss: 1.3785, Accuracy: 0.4763\n  Batch 250/889 - Loss: 1.3656, Accuracy: 0.4814\n  Batch 260/889 - Loss: 1.3540, Accuracy: 0.4869\n  Batch 270/889 - Loss: 1.3443, Accuracy: 0.4908\n  Batch 280/889 - Loss: 1.3324, Accuracy: 0.4966\n  Batch 290/889 - Loss: 1.3207, Accuracy: 0.5021\n  Batch 300/889 - Loss: 1.3110, Accuracy: 0.5062\n  Batch 310/889 - Loss: 1.3003, Accuracy: 0.5110\n  Batch 320/889 - Loss: 1.2898, Accuracy: 0.5154\n  Batch 330/889 - Loss: 1.2801, Accuracy: 0.5196\n  Batch 340/889 - Loss: 1.2721, Accuracy: 0.5226\n  Batch 350/889 - Loss: 1.2626, Accuracy: 0.5264\n  Batch 360/889 - Loss: 1.2531, Accuracy: 0.5300\n  Batch 370/889 - Loss: 1.2444, Accuracy: 0.5334\n  Batch 380/889 - Loss: 1.2361, Accuracy: 0.5371\n  Batch 390/889 - Loss: 1.2287, Accuracy: 0.5404\n  Batch 400/889 - Loss: 1.2210, Accuracy: 0.5440\n  Batch 410/889 - Loss: 1.2136, Accuracy: 0.5470\n  Batch 420/889 - Loss: 1.2055, Accuracy: 0.5500\n  Batch 430/889 - Loss: 1.1989, Accuracy: 0.5524\n  Batch 440/889 - Loss: 1.1926, Accuracy: 0.5550\n  Batch 450/889 - Loss: 1.1867, Accuracy: 0.5574\n  Batch 460/889 - Loss: 1.1792, Accuracy: 0.5604\n  Batch 470/889 - Loss: 1.1712, Accuracy: 0.5636\n  Batch 480/889 - Loss: 1.1649, Accuracy: 0.5662\n  Batch 490/889 - Loss: 1.1581, Accuracy: 0.5690\n  Batch 500/889 - Loss: 1.1541, Accuracy: 0.5709\n  Batch 510/889 - Loss: 1.1483, Accuracy: 0.5735\n  Batch 520/889 - Loss: 1.1421, Accuracy: 0.5758\n  Batch 530/889 - Loss: 1.1365, Accuracy: 0.5783\n  Batch 540/889 - Loss: 1.1313, Accuracy: 0.5806\n  Batch 550/889 - Loss: 1.1268, Accuracy: 0.5825\n  Batch 560/889 - Loss: 1.1208, Accuracy: 0.5849\n  Batch 570/889 - Loss: 1.1156, Accuracy: 0.5871\n  Batch 580/889 - Loss: 1.1100, Accuracy: 0.5897\n  Batch 590/889 - Loss: 1.1038, Accuracy: 0.5924\n  Batch 600/889 - Loss: 1.0979, Accuracy: 0.5949\n  Batch 610/889 - Loss: 1.0931, Accuracy: 0.5968\n  Batch 620/889 - Loss: 1.0896, Accuracy: 0.5980\n  Batch 630/889 - Loss: 1.0845, Accuracy: 0.5998\n  Batch 640/889 - Loss: 1.0805, Accuracy: 0.6014\n  Batch 650/889 - Loss: 1.0770, Accuracy: 0.6026\n  Batch 660/889 - Loss: 1.0718, Accuracy: 0.6050\n  Batch 670/889 - Loss: 1.0671, Accuracy: 0.6068\n  Batch 680/889 - Loss: 1.0623, Accuracy: 0.6087\n  Batch 690/889 - Loss: 1.0587, Accuracy: 0.6101\n  Batch 700/889 - Loss: 1.0535, Accuracy: 0.6124\n  Batch 710/889 - Loss: 1.0485, Accuracy: 0.6143\n  Batch 720/889 - Loss: 1.0439, Accuracy: 0.6161\n  Batch 730/889 - Loss: 1.0394, Accuracy: 0.6178\n  Batch 740/889 - Loss: 1.0343, Accuracy: 0.6196\n  Batch 750/889 - Loss: 1.0305, Accuracy: 0.6212\n  Batch 760/889 - Loss: 1.0266, Accuracy: 0.6226\n  Batch 770/889 - Loss: 1.0225, Accuracy: 0.6243\n  Batch 780/889 - Loss: 1.0192, Accuracy: 0.6257\n  Batch 790/889 - Loss: 1.0161, Accuracy: 0.6269\n  Batch 800/889 - Loss: 1.0123, Accuracy: 0.6284\n  Batch 810/889 - Loss: 1.0096, Accuracy: 0.6293\n  Batch 820/889 - Loss: 1.0058, Accuracy: 0.6307\n  Batch 830/889 - Loss: 1.0016, Accuracy: 0.6324\n  Batch 840/889 - Loss: 0.9977, Accuracy: 0.6339\n  Batch 850/889 - Loss: 0.9948, Accuracy: 0.6351\n  Batch 860/889 - Loss: 0.9910, Accuracy: 0.6365\n  Batch 870/889 - Loss: 0.9878, Accuracy: 0.6378\n  Batch 880/889 - Loss: 0.9844, Accuracy: 0.6392\n\nKết quả Epoch 1:\n  Loss trung bình: 0.9819\n  Độ chính xác trung bình: 0.6401\n  -> Cập nhật mô hình tốt nhất.\n\n===== Epoch 2 / 10 =====\n  Batch 10/889 - Loss: 0.5855, Accuracy: 0.8168\n  Batch 20/889 - Loss: 0.6187, Accuracy: 0.7939\n  Batch 30/889 - Loss: 0.6432, Accuracy: 0.7807\n  Batch 40/889 - Loss: 0.6551, Accuracy: 0.7755\n  Batch 50/889 - Loss: 0.6483, Accuracy: 0.7748\n  Batch 60/889 - Loss: 0.6526, Accuracy: 0.7702\n  Batch 70/889 - Loss: 0.6497, Accuracy: 0.7700\n  Batch 80/889 - Loss: 0.6485, Accuracy: 0.7703\n  Batch 90/889 - Loss: 0.6488, Accuracy: 0.7709\n  Batch 100/889 - Loss: 0.6480, Accuracy: 0.7729\n  Batch 110/889 - Loss: 0.6509, Accuracy: 0.7708\n  Batch 120/889 - Loss: 0.6470, Accuracy: 0.7729\n  Batch 130/889 - Loss: 0.6460, Accuracy: 0.7741\n  Batch 140/889 - Loss: 0.6456, Accuracy: 0.7755\n  Batch 150/889 - Loss: 0.6487, Accuracy: 0.7727\n  Batch 160/889 - Loss: 0.6468, Accuracy: 0.7734\n  Batch 170/889 - Loss: 0.6443, Accuracy: 0.7739\n  Batch 180/889 - Loss: 0.6393, Accuracy: 0.7754\n  Batch 190/889 - Loss: 0.6387, Accuracy: 0.7757\n  Batch 200/889 - Loss: 0.6358, Accuracy: 0.7764\n  Batch 210/889 - Loss: 0.6348, Accuracy: 0.7767\n  Batch 220/889 - Loss: 0.6338, Accuracy: 0.7774\n  Batch 230/889 - Loss: 0.6311, Accuracy: 0.7785\n  Batch 240/889 - Loss: 0.6319, Accuracy: 0.7781\n  Batch 250/889 - Loss: 0.6316, Accuracy: 0.7779\n  Batch 260/889 - Loss: 0.6297, Accuracy: 0.7782\n  Batch 270/889 - Loss: 0.6289, Accuracy: 0.7780\n  Batch 280/889 - Loss: 0.6285, Accuracy: 0.7776\n  Batch 290/889 - Loss: 0.6270, Accuracy: 0.7784\n  Batch 300/889 - Loss: 0.6272, Accuracy: 0.7783\n  Batch 310/889 - Loss: 0.6253, Accuracy: 0.7787\n  Batch 320/889 - Loss: 0.6265, Accuracy: 0.7778\n  Batch 330/889 - Loss: 0.6244, Accuracy: 0.7785\n  Batch 340/889 - Loss: 0.6240, Accuracy: 0.7788\n  Batch 350/889 - Loss: 0.6241, Accuracy: 0.7783\n  Batch 360/889 - Loss: 0.6233, Accuracy: 0.7784\n  Batch 370/889 - Loss: 0.6225, Accuracy: 0.7787\n  Batch 380/889 - Loss: 0.6226, Accuracy: 0.7784\n  Batch 390/889 - Loss: 0.6224, Accuracy: 0.7785\n  Batch 400/889 - Loss: 0.6210, Accuracy: 0.7789\n  Batch 410/889 - Loss: 0.6214, Accuracy: 0.7782\n  Batch 420/889 - Loss: 0.6203, Accuracy: 0.7785\n  Batch 430/889 - Loss: 0.6201, Accuracy: 0.7782\n  Batch 440/889 - Loss: 0.6183, Accuracy: 0.7788\n  Batch 450/889 - Loss: 0.6181, Accuracy: 0.7785\n  Batch 460/889 - Loss: 0.6168, Accuracy: 0.7793\n  Batch 470/889 - Loss: 0.6174, Accuracy: 0.7788\n  Batch 480/889 - Loss: 0.6167, Accuracy: 0.7794\n  Batch 490/889 - Loss: 0.6166, Accuracy: 0.7793\n  Batch 500/889 - Loss: 0.6163, Accuracy: 0.7794\n  Batch 510/889 - Loss: 0.6154, Accuracy: 0.7798\n  Batch 520/889 - Loss: 0.6146, Accuracy: 0.7800\n  Batch 530/889 - Loss: 0.6144, Accuracy: 0.7800\n  Batch 540/889 - Loss: 0.6133, Accuracy: 0.7801\n  Batch 550/889 - Loss: 0.6134, Accuracy: 0.7803\n  Batch 560/889 - Loss: 0.6129, Accuracy: 0.7806\n  Batch 570/889 - Loss: 0.6121, Accuracy: 0.7811\n  Batch 580/889 - Loss: 0.6110, Accuracy: 0.7813\n  Batch 590/889 - Loss: 0.6098, Accuracy: 0.7815\n  Batch 600/889 - Loss: 0.6085, Accuracy: 0.7819\n  Batch 610/889 - Loss: 0.6077, Accuracy: 0.7821\n  Batch 620/889 - Loss: 0.6070, Accuracy: 0.7822\n  Batch 630/889 - Loss: 0.6068, Accuracy: 0.7821\n  Batch 640/889 - Loss: 0.6058, Accuracy: 0.7822\n  Batch 650/889 - Loss: 0.6056, Accuracy: 0.7823\n  Batch 660/889 - Loss: 0.6050, Accuracy: 0.7823\n  Batch 670/889 - Loss: 0.6048, Accuracy: 0.7823\n  Batch 680/889 - Loss: 0.6034, Accuracy: 0.7825\n  Batch 690/889 - Loss: 0.6020, Accuracy: 0.7830\n  Batch 700/889 - Loss: 0.6013, Accuracy: 0.7836\n  Batch 710/889 - Loss: 0.6010, Accuracy: 0.7834\n  Batch 720/889 - Loss: 0.6004, Accuracy: 0.7837\n  Batch 730/889 - Loss: 0.5997, Accuracy: 0.7840\n  Batch 740/889 - Loss: 0.5990, Accuracy: 0.7842\n  Batch 750/889 - Loss: 0.5977, Accuracy: 0.7846\n  Batch 760/889 - Loss: 0.5974, Accuracy: 0.7848\n  Batch 770/889 - Loss: 0.5972, Accuracy: 0.7849\n  Batch 780/889 - Loss: 0.5963, Accuracy: 0.7852\n  Batch 790/889 - Loss: 0.5964, Accuracy: 0.7851\n  Batch 800/889 - Loss: 0.5957, Accuracy: 0.7854\n  Batch 810/889 - Loss: 0.5945, Accuracy: 0.7858\n  Batch 820/889 - Loss: 0.5934, Accuracy: 0.7861\n  Batch 830/889 - Loss: 0.5926, Accuracy: 0.7862\n  Batch 840/889 - Loss: 0.5918, Accuracy: 0.7866\n  Batch 850/889 - Loss: 0.5914, Accuracy: 0.7869\n  Batch 860/889 - Loss: 0.5904, Accuracy: 0.7872\n  Batch 870/889 - Loss: 0.5895, Accuracy: 0.7876\n  Batch 880/889 - Loss: 0.5890, Accuracy: 0.7877\n\nKết quả Epoch 2:\n  Loss trung bình: 0.5885\n  Độ chính xác trung bình: 0.7879\n  -> Cập nhật mô hình tốt nhất.\n\n===== Epoch 3 / 10 =====\n  Batch 10/889 - Loss: 0.5445, Accuracy: 0.8082\n  Batch 20/889 - Loss: 0.4972, Accuracy: 0.8304\n  Batch 30/889 - Loss: 0.5262, Accuracy: 0.8170\n  Batch 40/889 - Loss: 0.5257, Accuracy: 0.8152\n  Batch 50/889 - Loss: 0.5343, Accuracy: 0.8085\n  Batch 60/889 - Loss: 0.5244, Accuracy: 0.8099\n  Batch 70/889 - Loss: 0.5143, Accuracy: 0.8125\n  Batch 80/889 - Loss: 0.5153, Accuracy: 0.8115\n  Batch 90/889 - Loss: 0.5098, Accuracy: 0.8140\n  Batch 100/889 - Loss: 0.5155, Accuracy: 0.8125\n  Batch 110/889 - Loss: 0.5095, Accuracy: 0.8140\n  Batch 120/889 - Loss: 0.5109, Accuracy: 0.8139\n  Batch 130/889 - Loss: 0.5086, Accuracy: 0.8152\n  Batch 140/889 - Loss: 0.5052, Accuracy: 0.8163\n  Batch 150/889 - Loss: 0.5010, Accuracy: 0.8174\n  Batch 160/889 - Loss: 0.5022, Accuracy: 0.8172\n  Batch 170/889 - Loss: 0.5010, Accuracy: 0.8175\n  Batch 180/889 - Loss: 0.5007, Accuracy: 0.8179\n  Batch 190/889 - Loss: 0.4986, Accuracy: 0.8175\n  Batch 200/889 - Loss: 0.4981, Accuracy: 0.8175\n  Batch 210/889 - Loss: 0.5005, Accuracy: 0.8163\n  Batch 220/889 - Loss: 0.4985, Accuracy: 0.8167\n  Batch 230/889 - Loss: 0.4981, Accuracy: 0.8166\n  Batch 240/889 - Loss: 0.4985, Accuracy: 0.8170\n  Batch 250/889 - Loss: 0.4988, Accuracy: 0.8165\n  Batch 260/889 - Loss: 0.4989, Accuracy: 0.8169\n  Batch 270/889 - Loss: 0.4990, Accuracy: 0.8167\n  Batch 280/889 - Loss: 0.4972, Accuracy: 0.8173\n  Batch 290/889 - Loss: 0.4967, Accuracy: 0.8169\n  Batch 300/889 - Loss: 0.4957, Accuracy: 0.8168\n  Batch 310/889 - Loss: 0.4939, Accuracy: 0.8174\n  Batch 320/889 - Loss: 0.4935, Accuracy: 0.8175\n  Batch 330/889 - Loss: 0.4914, Accuracy: 0.8184\n  Batch 340/889 - Loss: 0.4892, Accuracy: 0.8195\n  Batch 350/889 - Loss: 0.4896, Accuracy: 0.8192\n  Batch 360/889 - Loss: 0.4877, Accuracy: 0.8199\n  Batch 370/889 - Loss: 0.4862, Accuracy: 0.8205\n  Batch 380/889 - Loss: 0.4866, Accuracy: 0.8203\n  Batch 390/889 - Loss: 0.4866, Accuracy: 0.8200\n  Batch 400/889 - Loss: 0.4859, Accuracy: 0.8202\n  Batch 410/889 - Loss: 0.4860, Accuracy: 0.8202\n  Batch 420/889 - Loss: 0.4863, Accuracy: 0.8201\n  Batch 430/889 - Loss: 0.4865, Accuracy: 0.8198\n  Batch 440/889 - Loss: 0.4859, Accuracy: 0.8199\n  Batch 450/889 - Loss: 0.4847, Accuracy: 0.8203\n  Batch 460/889 - Loss: 0.4869, Accuracy: 0.8193\n  Batch 470/889 - Loss: 0.4877, Accuracy: 0.8189\n  Batch 480/889 - Loss: 0.4876, Accuracy: 0.8191\n  Batch 490/889 - Loss: 0.4870, Accuracy: 0.8196\n  Batch 500/889 - Loss: 0.4862, Accuracy: 0.8201\n  Batch 510/889 - Loss: 0.4852, Accuracy: 0.8207\n  Batch 520/889 - Loss: 0.4845, Accuracy: 0.8209\n  Batch 530/889 - Loss: 0.4839, Accuracy: 0.8214\n  Batch 540/889 - Loss: 0.4849, Accuracy: 0.8209\n  Batch 550/889 - Loss: 0.4845, Accuracy: 0.8210\n  Batch 560/889 - Loss: 0.4841, Accuracy: 0.8212\n  Batch 570/889 - Loss: 0.4845, Accuracy: 0.8210\n  Batch 580/889 - Loss: 0.4844, Accuracy: 0.8210\n  Batch 590/889 - Loss: 0.4839, Accuracy: 0.8213\n  Batch 600/889 - Loss: 0.4837, Accuracy: 0.8212\n  Batch 610/889 - Loss: 0.4831, Accuracy: 0.8213\n  Batch 620/889 - Loss: 0.4830, Accuracy: 0.8214\n  Batch 630/889 - Loss: 0.4828, Accuracy: 0.8216\n  Batch 640/889 - Loss: 0.4824, Accuracy: 0.8215\n  Batch 650/889 - Loss: 0.4822, Accuracy: 0.8214\n  Batch 660/889 - Loss: 0.4823, Accuracy: 0.8213\n  Batch 670/889 - Loss: 0.4821, Accuracy: 0.8216\n  Batch 680/889 - Loss: 0.4822, Accuracy: 0.8213\n  Batch 690/889 - Loss: 0.4821, Accuracy: 0.8214\n  Batch 700/889 - Loss: 0.4810, Accuracy: 0.8219\n  Batch 710/889 - Loss: 0.4798, Accuracy: 0.8223\n  Batch 720/889 - Loss: 0.4794, Accuracy: 0.8226\n  Batch 730/889 - Loss: 0.4783, Accuracy: 0.8231\n  Batch 740/889 - Loss: 0.4773, Accuracy: 0.8236\n  Batch 750/889 - Loss: 0.4774, Accuracy: 0.8235\n  Batch 760/889 - Loss: 0.4761, Accuracy: 0.8240\n  Batch 770/889 - Loss: 0.4757, Accuracy: 0.8241\n  Batch 780/889 - Loss: 0.4739, Accuracy: 0.8248\n  Batch 790/889 - Loss: 0.4729, Accuracy: 0.8252\n  Batch 800/889 - Loss: 0.4725, Accuracy: 0.8253\n  Batch 810/889 - Loss: 0.4724, Accuracy: 0.8252\n  Batch 820/889 - Loss: 0.4720, Accuracy: 0.8253\n  Batch 830/889 - Loss: 0.4716, Accuracy: 0.8254\n  Batch 840/889 - Loss: 0.4714, Accuracy: 0.8256\n  Batch 850/889 - Loss: 0.4710, Accuracy: 0.8258\n  Batch 860/889 - Loss: 0.4702, Accuracy: 0.8262\n  Batch 870/889 - Loss: 0.4705, Accuracy: 0.8260\n  Batch 880/889 - Loss: 0.4700, Accuracy: 0.8261\n\nKết quả Epoch 3:\n  Loss trung bình: 0.4699\n  Độ chính xác trung bình: 0.8262\n  -> Cập nhật mô hình tốt nhất.\n\n===== Epoch 4 / 10 =====\n  Batch 10/889 - Loss: 0.3920, Accuracy: 0.8438\n  Batch 20/889 - Loss: 0.3756, Accuracy: 0.8571\n  Batch 30/889 - Loss: 0.3724, Accuracy: 0.8579\n  Batch 40/889 - Loss: 0.3756, Accuracy: 0.8579\n  Batch 50/889 - Loss: 0.3778, Accuracy: 0.8578\n  Batch 60/889 - Loss: 0.3739, Accuracy: 0.8609\n  Batch 70/889 - Loss: 0.3818, Accuracy: 0.8574\n  Batch 80/889 - Loss: 0.3890, Accuracy: 0.8561\n  Batch 90/889 - Loss: 0.3873, Accuracy: 0.8563\n  Batch 100/889 - Loss: 0.3907, Accuracy: 0.8555\n  Batch 110/889 - Loss: 0.3929, Accuracy: 0.8556\n  Batch 120/889 - Loss: 0.3925, Accuracy: 0.8565\n  Batch 130/889 - Loss: 0.3941, Accuracy: 0.8564\n  Batch 140/889 - Loss: 0.3949, Accuracy: 0.8564\n  Batch 150/889 - Loss: 0.3985, Accuracy: 0.8548\n  Batch 160/889 - Loss: 0.3987, Accuracy: 0.8544\n  Batch 170/889 - Loss: 0.3984, Accuracy: 0.8534\n  Batch 180/889 - Loss: 0.3979, Accuracy: 0.8539\n  Batch 190/889 - Loss: 0.3977, Accuracy: 0.8538\n  Batch 200/889 - Loss: 0.3965, Accuracy: 0.8537\n  Batch 210/889 - Loss: 0.3962, Accuracy: 0.8533\n  Batch 220/889 - Loss: 0.3958, Accuracy: 0.8537\n  Batch 230/889 - Loss: 0.3938, Accuracy: 0.8547\n  Batch 240/889 - Loss: 0.3948, Accuracy: 0.8545\n  Batch 250/889 - Loss: 0.3952, Accuracy: 0.8543\n  Batch 260/889 - Loss: 0.3963, Accuracy: 0.8536\n  Batch 270/889 - Loss: 0.3974, Accuracy: 0.8534\n  Batch 280/889 - Loss: 0.3979, Accuracy: 0.8536\n  Batch 290/889 - Loss: 0.3982, Accuracy: 0.8536\n  Batch 300/889 - Loss: 0.3992, Accuracy: 0.8530\n  Batch 310/889 - Loss: 0.4003, Accuracy: 0.8527\n  Batch 320/889 - Loss: 0.4007, Accuracy: 0.8527\n  Batch 330/889 - Loss: 0.4009, Accuracy: 0.8523\n  Batch 340/889 - Loss: 0.4014, Accuracy: 0.8520\n  Batch 350/889 - Loss: 0.4014, Accuracy: 0.8518\n  Batch 360/889 - Loss: 0.4017, Accuracy: 0.8518\n  Batch 370/889 - Loss: 0.4006, Accuracy: 0.8524\n  Batch 380/889 - Loss: 0.4011, Accuracy: 0.8520\n  Batch 390/889 - Loss: 0.4023, Accuracy: 0.8515\n  Batch 400/889 - Loss: 0.4010, Accuracy: 0.8520\n  Batch 410/889 - Loss: 0.4023, Accuracy: 0.8512\n  Batch 420/889 - Loss: 0.4027, Accuracy: 0.8508\n  Batch 430/889 - Loss: 0.4030, Accuracy: 0.8502\n  Batch 440/889 - Loss: 0.4025, Accuracy: 0.8505\n  Batch 450/889 - Loss: 0.4023, Accuracy: 0.8510\n  Batch 460/889 - Loss: 0.4021, Accuracy: 0.8510\n  Batch 470/889 - Loss: 0.4018, Accuracy: 0.8509\n  Batch 480/889 - Loss: 0.4018, Accuracy: 0.8509\n  Batch 490/889 - Loss: 0.4030, Accuracy: 0.8504\n  Batch 500/889 - Loss: 0.4039, Accuracy: 0.8505\n  Batch 510/889 - Loss: 0.4048, Accuracy: 0.8503\n  Batch 520/889 - Loss: 0.4052, Accuracy: 0.8502\n  Batch 530/889 - Loss: 0.4046, Accuracy: 0.8504\n  Batch 540/889 - Loss: 0.4052, Accuracy: 0.8501\n  Batch 550/889 - Loss: 0.4053, Accuracy: 0.8501\n  Batch 560/889 - Loss: 0.4053, Accuracy: 0.8501\n  Batch 570/889 - Loss: 0.4058, Accuracy: 0.8499\n  Batch 580/889 - Loss: 0.4062, Accuracy: 0.8498\n  Batch 590/889 - Loss: 0.4052, Accuracy: 0.8501\n  Batch 600/889 - Loss: 0.4048, Accuracy: 0.8503\n  Batch 610/889 - Loss: 0.4051, Accuracy: 0.8503\n  Batch 620/889 - Loss: 0.4042, Accuracy: 0.8507\n  Batch 630/889 - Loss: 0.4039, Accuracy: 0.8507\n  Batch 640/889 - Loss: 0.4033, Accuracy: 0.8510\n  Batch 650/889 - Loss: 0.4033, Accuracy: 0.8510\n  Batch 660/889 - Loss: 0.4029, Accuracy: 0.8513\n  Batch 670/889 - Loss: 0.4030, Accuracy: 0.8515\n  Batch 680/889 - Loss: 0.4034, Accuracy: 0.8512\n  Batch 690/889 - Loss: 0.4033, Accuracy: 0.8513\n  Batch 700/889 - Loss: 0.4040, Accuracy: 0.8509\n  Batch 710/889 - Loss: 0.4036, Accuracy: 0.8511\n  Batch 720/889 - Loss: 0.4037, Accuracy: 0.8509\n  Batch 730/889 - Loss: 0.4035, Accuracy: 0.8510\n  Batch 740/889 - Loss: 0.4029, Accuracy: 0.8512\n  Batch 750/889 - Loss: 0.4023, Accuracy: 0.8514\n  Batch 760/889 - Loss: 0.4019, Accuracy: 0.8516\n  Batch 770/889 - Loss: 0.4015, Accuracy: 0.8517\n  Batch 780/889 - Loss: 0.4016, Accuracy: 0.8517\n  Batch 790/889 - Loss: 0.4014, Accuracy: 0.8518\n  Batch 800/889 - Loss: 0.4016, Accuracy: 0.8518\n  Batch 810/889 - Loss: 0.4013, Accuracy: 0.8518\n  Batch 820/889 - Loss: 0.4008, Accuracy: 0.8519\n  Batch 830/889 - Loss: 0.4009, Accuracy: 0.8517\n  Batch 840/889 - Loss: 0.4001, Accuracy: 0.8519\n  Batch 850/889 - Loss: 0.3996, Accuracy: 0.8521\n  Batch 860/889 - Loss: 0.3992, Accuracy: 0.8523\n  Batch 870/889 - Loss: 0.3990, Accuracy: 0.8524\n  Batch 880/889 - Loss: 0.3995, Accuracy: 0.8521\n\nKết quả Epoch 4:\n  Loss trung bình: 0.3990\n  Độ chính xác trung bình: 0.8524\n  -> Cập nhật mô hình tốt nhất.\n\n===== Epoch 5 / 10 =====\n  Batch 10/889 - Loss: 0.3579, Accuracy: 0.8580\n  Batch 20/889 - Loss: 0.3770, Accuracy: 0.8482\n  Batch 30/889 - Loss: 0.3764, Accuracy: 0.8528\n  Batch 40/889 - Loss: 0.3704, Accuracy: 0.8563\n  Batch 50/889 - Loss: 0.3748, Accuracy: 0.8551\n  Batch 60/889 - Loss: 0.3765, Accuracy: 0.8535\n  Batch 70/889 - Loss: 0.3793, Accuracy: 0.8504\n  Batch 80/889 - Loss: 0.3794, Accuracy: 0.8515\n  Batch 90/889 - Loss: 0.3742, Accuracy: 0.8549\n  Batch 100/889 - Loss: 0.3739, Accuracy: 0.8557\n  Batch 110/889 - Loss: 0.3710, Accuracy: 0.8574\n  Batch 120/889 - Loss: 0.3667, Accuracy: 0.8592\n  Batch 130/889 - Loss: 0.3656, Accuracy: 0.8596\n  Batch 140/889 - Loss: 0.3623, Accuracy: 0.8608\n  Batch 150/889 - Loss: 0.3614, Accuracy: 0.8618\n  Batch 160/889 - Loss: 0.3591, Accuracy: 0.8627\n  Batch 170/889 - Loss: 0.3582, Accuracy: 0.8639\n  Batch 180/889 - Loss: 0.3578, Accuracy: 0.8643\n  Batch 190/889 - Loss: 0.3584, Accuracy: 0.8642\n  Batch 200/889 - Loss: 0.3582, Accuracy: 0.8651\n  Batch 210/889 - Loss: 0.3551, Accuracy: 0.8669\n  Batch 220/889 - Loss: 0.3565, Accuracy: 0.8665\n  Batch 230/889 - Loss: 0.3567, Accuracy: 0.8663\n  Batch 240/889 - Loss: 0.3545, Accuracy: 0.8671\n  Batch 250/889 - Loss: 0.3542, Accuracy: 0.8673\n  Batch 260/889 - Loss: 0.3535, Accuracy: 0.8677\n  Batch 270/889 - Loss: 0.3547, Accuracy: 0.8674\n  Batch 280/889 - Loss: 0.3554, Accuracy: 0.8669\n  Batch 290/889 - Loss: 0.3545, Accuracy: 0.8665\n  Batch 300/889 - Loss: 0.3548, Accuracy: 0.8667\n  Batch 310/889 - Loss: 0.3556, Accuracy: 0.8663\n  Batch 320/889 - Loss: 0.3568, Accuracy: 0.8660\n  Batch 330/889 - Loss: 0.3577, Accuracy: 0.8654\n  Batch 340/889 - Loss: 0.3570, Accuracy: 0.8657\n  Batch 350/889 - Loss: 0.3574, Accuracy: 0.8655\n  Batch 360/889 - Loss: 0.3584, Accuracy: 0.8651\n  Batch 370/889 - Loss: 0.3587, Accuracy: 0.8653\n  Batch 380/889 - Loss: 0.3577, Accuracy: 0.8660\n  Batch 390/889 - Loss: 0.3581, Accuracy: 0.8660\n  Batch 400/889 - Loss: 0.3586, Accuracy: 0.8660\n  Batch 410/889 - Loss: 0.3593, Accuracy: 0.8659\n  Batch 420/889 - Loss: 0.3587, Accuracy: 0.8663\n  Batch 430/889 - Loss: 0.3594, Accuracy: 0.8661\n  Batch 440/889 - Loss: 0.3600, Accuracy: 0.8659\n  Batch 450/889 - Loss: 0.3595, Accuracy: 0.8660\n  Batch 460/889 - Loss: 0.3589, Accuracy: 0.8660\n  Batch 470/889 - Loss: 0.3590, Accuracy: 0.8660\n  Batch 480/889 - Loss: 0.3591, Accuracy: 0.8659\n  Batch 490/889 - Loss: 0.3601, Accuracy: 0.8656\n  Batch 500/889 - Loss: 0.3599, Accuracy: 0.8655\n  Batch 510/889 - Loss: 0.3605, Accuracy: 0.8652\n  Batch 520/889 - Loss: 0.3606, Accuracy: 0.8653\n  Batch 530/889 - Loss: 0.3602, Accuracy: 0.8652\n  Batch 540/889 - Loss: 0.3593, Accuracy: 0.8656\n  Batch 550/889 - Loss: 0.3593, Accuracy: 0.8658\n  Batch 560/889 - Loss: 0.3586, Accuracy: 0.8659\n  Batch 570/889 - Loss: 0.3595, Accuracy: 0.8659\n  Batch 580/889 - Loss: 0.3593, Accuracy: 0.8660\n  Batch 590/889 - Loss: 0.3590, Accuracy: 0.8664\n  Batch 600/889 - Loss: 0.3588, Accuracy: 0.8667\n  Batch 610/889 - Loss: 0.3584, Accuracy: 0.8669\n  Batch 620/889 - Loss: 0.3582, Accuracy: 0.8669\n  Batch 630/889 - Loss: 0.3585, Accuracy: 0.8667\n  Batch 640/889 - Loss: 0.3583, Accuracy: 0.8667\n  Batch 650/889 - Loss: 0.3585, Accuracy: 0.8665\n  Batch 660/889 - Loss: 0.3579, Accuracy: 0.8667\n  Batch 670/889 - Loss: 0.3572, Accuracy: 0.8669\n  Batch 680/889 - Loss: 0.3565, Accuracy: 0.8671\n  Batch 690/889 - Loss: 0.3560, Accuracy: 0.8674\n  Batch 700/889 - Loss: 0.3562, Accuracy: 0.8674\n  Batch 710/889 - Loss: 0.3569, Accuracy: 0.8669\n  Batch 720/889 - Loss: 0.3570, Accuracy: 0.8668\n  Batch 730/889 - Loss: 0.3567, Accuracy: 0.8670\n  Batch 740/889 - Loss: 0.3570, Accuracy: 0.8668\n  Batch 750/889 - Loss: 0.3566, Accuracy: 0.8669\n  Batch 760/889 - Loss: 0.3562, Accuracy: 0.8672\n  Batch 770/889 - Loss: 0.3565, Accuracy: 0.8670\n  Batch 780/889 - Loss: 0.3559, Accuracy: 0.8673\n  Batch 790/889 - Loss: 0.3549, Accuracy: 0.8677\n  Batch 800/889 - Loss: 0.3544, Accuracy: 0.8680\n  Batch 810/889 - Loss: 0.3549, Accuracy: 0.8678\n  Batch 820/889 - Loss: 0.3549, Accuracy: 0.8679\n  Batch 830/889 - Loss: 0.3551, Accuracy: 0.8677\n  Batch 840/889 - Loss: 0.3549, Accuracy: 0.8677\n  Batch 850/889 - Loss: 0.3550, Accuracy: 0.8675\n  Batch 860/889 - Loss: 0.3546, Accuracy: 0.8676\n  Batch 870/889 - Loss: 0.3543, Accuracy: 0.8677\n  Batch 880/889 - Loss: 0.3543, Accuracy: 0.8675\n\nKết quả Epoch 5:\n  Loss trung bình: 0.3543\n  Độ chính xác trung bình: 0.8676\n  -> Cập nhật mô hình tốt nhất.\n\n===== Epoch 6 / 10 =====\n  Batch 10/889 - Loss: 0.3573, Accuracy: 0.8750\n  Batch 20/889 - Loss: 0.3496, Accuracy: 0.8750\n  Batch 30/889 - Loss: 0.3452, Accuracy: 0.8750\n  Batch 40/889 - Loss: 0.3575, Accuracy: 0.8681\n  Batch 50/889 - Loss: 0.3452, Accuracy: 0.8732\n  Batch 60/889 - Loss: 0.3499, Accuracy: 0.8714\n  Batch 70/889 - Loss: 0.3440, Accuracy: 0.8732\n  Batch 80/889 - Loss: 0.3403, Accuracy: 0.8750\n  Batch 90/889 - Loss: 0.3332, Accuracy: 0.8777\n  Batch 100/889 - Loss: 0.3331, Accuracy: 0.8769\n  Batch 110/889 - Loss: 0.3331, Accuracy: 0.8774\n  Batch 120/889 - Loss: 0.3309, Accuracy: 0.8780\n  Batch 130/889 - Loss: 0.3287, Accuracy: 0.8785\n  Batch 140/889 - Loss: 0.3267, Accuracy: 0.8791\n  Batch 150/889 - Loss: 0.3281, Accuracy: 0.8783\n  Batch 160/889 - Loss: 0.3292, Accuracy: 0.8768\n  Batch 170/889 - Loss: 0.3290, Accuracy: 0.8774\n  Batch 180/889 - Loss: 0.3310, Accuracy: 0.8768\n  Batch 190/889 - Loss: 0.3307, Accuracy: 0.8770\n  Batch 200/889 - Loss: 0.3304, Accuracy: 0.8769\n  Batch 210/889 - Loss: 0.3303, Accuracy: 0.8768\n  Batch 220/889 - Loss: 0.3296, Accuracy: 0.8768\n  Batch 230/889 - Loss: 0.3273, Accuracy: 0.8776\n  Batch 240/889 - Loss: 0.3276, Accuracy: 0.8775\n  Batch 250/889 - Loss: 0.3262, Accuracy: 0.8782\n  Batch 260/889 - Loss: 0.3255, Accuracy: 0.8789\n  Batch 270/889 - Loss: 0.3272, Accuracy: 0.8781\n  Batch 280/889 - Loss: 0.3278, Accuracy: 0.8779\n  Batch 290/889 - Loss: 0.3280, Accuracy: 0.8775\n  Batch 300/889 - Loss: 0.3284, Accuracy: 0.8778\n  Batch 310/889 - Loss: 0.3282, Accuracy: 0.8780\n  Batch 320/889 - Loss: 0.3284, Accuracy: 0.8777\n  Batch 330/889 - Loss: 0.3282, Accuracy: 0.8776\n  Batch 340/889 - Loss: 0.3297, Accuracy: 0.8771\n  Batch 350/889 - Loss: 0.3282, Accuracy: 0.8778\n  Batch 360/889 - Loss: 0.3277, Accuracy: 0.8779\n  Batch 370/889 - Loss: 0.3278, Accuracy: 0.8777\n  Batch 380/889 - Loss: 0.3274, Accuracy: 0.8778\n  Batch 390/889 - Loss: 0.3274, Accuracy: 0.8779\n  Batch 400/889 - Loss: 0.3282, Accuracy: 0.8776\n  Batch 410/889 - Loss: 0.3270, Accuracy: 0.8780\n  Batch 420/889 - Loss: 0.3263, Accuracy: 0.8782\n  Batch 430/889 - Loss: 0.3254, Accuracy: 0.8784\n  Batch 440/889 - Loss: 0.3243, Accuracy: 0.8787\n  Batch 450/889 - Loss: 0.3237, Accuracy: 0.8788\n  Batch 460/889 - Loss: 0.3237, Accuracy: 0.8788\n  Batch 470/889 - Loss: 0.3242, Accuracy: 0.8784\n  Batch 480/889 - Loss: 0.3245, Accuracy: 0.8782\n  Batch 490/889 - Loss: 0.3241, Accuracy: 0.8785\n  Batch 500/889 - Loss: 0.3235, Accuracy: 0.8786\n  Batch 510/889 - Loss: 0.3236, Accuracy: 0.8785\n  Batch 520/889 - Loss: 0.3239, Accuracy: 0.8783\n  Batch 530/889 - Loss: 0.3235, Accuracy: 0.8786\n  Batch 540/889 - Loss: 0.3236, Accuracy: 0.8786\n  Batch 550/889 - Loss: 0.3228, Accuracy: 0.8791\n  Batch 560/889 - Loss: 0.3226, Accuracy: 0.8791\n  Batch 570/889 - Loss: 0.3215, Accuracy: 0.8795\n  Batch 580/889 - Loss: 0.3212, Accuracy: 0.8797\n  Batch 590/889 - Loss: 0.3213, Accuracy: 0.8798\n  Batch 600/889 - Loss: 0.3211, Accuracy: 0.8798\n  Batch 610/889 - Loss: 0.3211, Accuracy: 0.8797\n  Batch 620/889 - Loss: 0.3208, Accuracy: 0.8797\n  Batch 630/889 - Loss: 0.3207, Accuracy: 0.8799\n  Batch 640/889 - Loss: 0.3203, Accuracy: 0.8800\n  Batch 650/889 - Loss: 0.3203, Accuracy: 0.8799\n  Batch 660/889 - Loss: 0.3207, Accuracy: 0.8796\n  Batch 670/889 - Loss: 0.3196, Accuracy: 0.8799\n  Batch 680/889 - Loss: 0.3198, Accuracy: 0.8798\n  Batch 690/889 - Loss: 0.3198, Accuracy: 0.8797\n  Batch 700/889 - Loss: 0.3199, Accuracy: 0.8797\n  Batch 710/889 - Loss: 0.3194, Accuracy: 0.8800\n  Batch 720/889 - Loss: 0.3193, Accuracy: 0.8802\n  Batch 730/889 - Loss: 0.3192, Accuracy: 0.8802\n  Batch 740/889 - Loss: 0.3195, Accuracy: 0.8801\n  Batch 750/889 - Loss: 0.3192, Accuracy: 0.8803\n  Batch 760/889 - Loss: 0.3189, Accuracy: 0.8805\n  Batch 770/889 - Loss: 0.3192, Accuracy: 0.8804\n  Batch 780/889 - Loss: 0.3187, Accuracy: 0.8807\n  Batch 790/889 - Loss: 0.3185, Accuracy: 0.8806\n  Batch 800/889 - Loss: 0.3189, Accuracy: 0.8803\n  Batch 810/889 - Loss: 0.3187, Accuracy: 0.8804\n  Batch 820/889 - Loss: 0.3187, Accuracy: 0.8804\n  Batch 830/889 - Loss: 0.3190, Accuracy: 0.8802\n  Batch 840/889 - Loss: 0.3192, Accuracy: 0.8801\n  Batch 850/889 - Loss: 0.3198, Accuracy: 0.8799\n  Batch 860/889 - Loss: 0.3200, Accuracy: 0.8797\n  Batch 870/889 - Loss: 0.3200, Accuracy: 0.8799\n  Batch 880/889 - Loss: 0.3199, Accuracy: 0.8800\n\nKết quả Epoch 6:\n  Loss trung bình: 0.3199\n  Độ chính xác trung bình: 0.8799\n  -> Cập nhật mô hình tốt nhất.\n\n===== Epoch 7 / 10 =====\n  Batch 10/889 - Loss: 0.3062, Accuracy: 0.8835\n  Batch 20/889 - Loss: 0.3155, Accuracy: 0.8824\n  Batch 30/889 - Loss: 0.3001, Accuracy: 0.8856\n  Batch 40/889 - Loss: 0.3004, Accuracy: 0.8861\n  Batch 50/889 - Loss: 0.2946, Accuracy: 0.8882\n  Batch 60/889 - Loss: 0.2922, Accuracy: 0.8888\n  Batch 70/889 - Loss: 0.2889, Accuracy: 0.8900\n  Batch 80/889 - Loss: 0.2870, Accuracy: 0.8885\n  Batch 90/889 - Loss: 0.2934, Accuracy: 0.8862\n  Batch 100/889 - Loss: 0.2942, Accuracy: 0.8857\n  Batch 110/889 - Loss: 0.2962, Accuracy: 0.8858\n  Batch 120/889 - Loss: 0.2941, Accuracy: 0.8873\n  Batch 130/889 - Loss: 0.2903, Accuracy: 0.8886\n  Batch 140/889 - Loss: 0.2884, Accuracy: 0.8895\n  Batch 150/889 - Loss: 0.2876, Accuracy: 0.8906\n  Batch 160/889 - Loss: 0.2901, Accuracy: 0.8903\n  Batch 170/889 - Loss: 0.2896, Accuracy: 0.8905\n  Batch 180/889 - Loss: 0.2891, Accuracy: 0.8911\n  Batch 190/889 - Loss: 0.2874, Accuracy: 0.8922\n  Batch 200/889 - Loss: 0.2895, Accuracy: 0.8916\n  Batch 210/889 - Loss: 0.2905, Accuracy: 0.8913\n  Batch 220/889 - Loss: 0.2905, Accuracy: 0.8916\n  Batch 230/889 - Loss: 0.2921, Accuracy: 0.8912\n  Batch 240/889 - Loss: 0.2918, Accuracy: 0.8914\n  Batch 250/889 - Loss: 0.2909, Accuracy: 0.8916\n  Batch 260/889 - Loss: 0.2911, Accuracy: 0.8916\n  Batch 270/889 - Loss: 0.2927, Accuracy: 0.8914\n  Batch 280/889 - Loss: 0.2935, Accuracy: 0.8911\n  Batch 290/889 - Loss: 0.2926, Accuracy: 0.8911\n  Batch 300/889 - Loss: 0.2927, Accuracy: 0.8912\n  Batch 310/889 - Loss: 0.2916, Accuracy: 0.8914\n  Batch 320/889 - Loss: 0.2920, Accuracy: 0.8913\n  Batch 330/889 - Loss: 0.2920, Accuracy: 0.8915\n  Batch 340/889 - Loss: 0.2937, Accuracy: 0.8907\n  Batch 350/889 - Loss: 0.2929, Accuracy: 0.8911\n  Batch 360/889 - Loss: 0.2921, Accuracy: 0.8911\n  Batch 370/889 - Loss: 0.2909, Accuracy: 0.8913\n  Batch 380/889 - Loss: 0.2914, Accuracy: 0.8911\n  Batch 390/889 - Loss: 0.2908, Accuracy: 0.8915\n  Batch 400/889 - Loss: 0.2912, Accuracy: 0.8914\n  Batch 410/889 - Loss: 0.2910, Accuracy: 0.8912\n  Batch 420/889 - Loss: 0.2914, Accuracy: 0.8912\n  Batch 430/889 - Loss: 0.2913, Accuracy: 0.8914\n  Batch 440/889 - Loss: 0.2916, Accuracy: 0.8914\n  Batch 450/889 - Loss: 0.2907, Accuracy: 0.8917\n  Batch 460/889 - Loss: 0.2913, Accuracy: 0.8915\n  Batch 470/889 - Loss: 0.2919, Accuracy: 0.8910\n  Batch 480/889 - Loss: 0.2912, Accuracy: 0.8910\n  Batch 490/889 - Loss: 0.2913, Accuracy: 0.8908\n  Batch 500/889 - Loss: 0.2913, Accuracy: 0.8909\n  Batch 510/889 - Loss: 0.2915, Accuracy: 0.8907\n  Batch 520/889 - Loss: 0.2918, Accuracy: 0.8906\n  Batch 530/889 - Loss: 0.2910, Accuracy: 0.8910\n  Batch 540/889 - Loss: 0.2911, Accuracy: 0.8910\n  Batch 550/889 - Loss: 0.2909, Accuracy: 0.8910\n  Batch 560/889 - Loss: 0.2904, Accuracy: 0.8912\n  Batch 570/889 - Loss: 0.2900, Accuracy: 0.8913\n  Batch 580/889 - Loss: 0.2906, Accuracy: 0.8911\n  Batch 590/889 - Loss: 0.2914, Accuracy: 0.8906\n  Batch 600/889 - Loss: 0.2915, Accuracy: 0.8905\n  Batch 610/889 - Loss: 0.2921, Accuracy: 0.8902\n  Batch 620/889 - Loss: 0.2921, Accuracy: 0.8902\n  Batch 630/889 - Loss: 0.2920, Accuracy: 0.8904\n  Batch 640/889 - Loss: 0.2918, Accuracy: 0.8906\n  Batch 650/889 - Loss: 0.2913, Accuracy: 0.8907\n  Batch 660/889 - Loss: 0.2908, Accuracy: 0.8908\n  Batch 670/889 - Loss: 0.2911, Accuracy: 0.8906\n  Batch 680/889 - Loss: 0.2914, Accuracy: 0.8906\n  Batch 690/889 - Loss: 0.2917, Accuracy: 0.8904\n  Batch 700/889 - Loss: 0.2915, Accuracy: 0.8903\n  Batch 710/889 - Loss: 0.2917, Accuracy: 0.8903\n  Batch 720/889 - Loss: 0.2917, Accuracy: 0.8904\n  Batch 730/889 - Loss: 0.2915, Accuracy: 0.8905\n  Batch 740/889 - Loss: 0.2916, Accuracy: 0.8907\n  Batch 750/889 - Loss: 0.2914, Accuracy: 0.8907\n  Batch 760/889 - Loss: 0.2921, Accuracy: 0.8904\n  Batch 770/889 - Loss: 0.2919, Accuracy: 0.8905\n  Batch 780/889 - Loss: 0.2919, Accuracy: 0.8904\n  Batch 790/889 - Loss: 0.2919, Accuracy: 0.8905\n  Batch 800/889 - Loss: 0.2919, Accuracy: 0.8905\n  Batch 810/889 - Loss: 0.2916, Accuracy: 0.8906\n  Batch 820/889 - Loss: 0.2917, Accuracy: 0.8907\n  Batch 830/889 - Loss: 0.2919, Accuracy: 0.8906\n  Batch 840/889 - Loss: 0.2913, Accuracy: 0.8909\n  Batch 850/889 - Loss: 0.2912, Accuracy: 0.8910\n  Batch 860/889 - Loss: 0.2914, Accuracy: 0.8910\n  Batch 870/889 - Loss: 0.2912, Accuracy: 0.8911\n  Batch 880/889 - Loss: 0.2913, Accuracy: 0.8911\n\nKết quả Epoch 7:\n  Loss trung bình: 0.2915\n  Độ chính xác trung bình: 0.8909\n  -> Cập nhật mô hình tốt nhất.\n\n===== Epoch 8 / 10 =====\n  Batch 10/889 - Loss: 0.2975, Accuracy: 0.8892\n  Batch 20/889 - Loss: 0.2878, Accuracy: 0.8899\n  Batch 30/889 - Loss: 0.2714, Accuracy: 0.9002\n  Batch 40/889 - Loss: 0.2753, Accuracy: 0.8963\n  Batch 50/889 - Loss: 0.2835, Accuracy: 0.8937\n  Batch 60/889 - Loss: 0.2837, Accuracy: 0.8919\n  Batch 70/889 - Loss: 0.2824, Accuracy: 0.8924\n  Batch 80/889 - Loss: 0.2806, Accuracy: 0.8929\n  Batch 90/889 - Loss: 0.2806, Accuracy: 0.8929\n  Batch 100/889 - Loss: 0.2769, Accuracy: 0.8942\n  Batch 110/889 - Loss: 0.2751, Accuracy: 0.8934\n  Batch 120/889 - Loss: 0.2738, Accuracy: 0.8946\n  Batch 130/889 - Loss: 0.2746, Accuracy: 0.8949\n  Batch 140/889 - Loss: 0.2731, Accuracy: 0.8961\n  Batch 150/889 - Loss: 0.2693, Accuracy: 0.8968\n  Batch 160/889 - Loss: 0.2667, Accuracy: 0.8986\n  Batch 170/889 - Loss: 0.2644, Accuracy: 0.8991\n  Batch 180/889 - Loss: 0.2638, Accuracy: 0.8995\n  Batch 190/889 - Loss: 0.2631, Accuracy: 0.9001\n  Batch 200/889 - Loss: 0.2616, Accuracy: 0.9006\n  Batch 210/889 - Loss: 0.2627, Accuracy: 0.9002\n  Batch 220/889 - Loss: 0.2641, Accuracy: 0.8997\n  Batch 230/889 - Loss: 0.2639, Accuracy: 0.9000\n  Batch 240/889 - Loss: 0.2645, Accuracy: 0.8995\n  Batch 250/889 - Loss: 0.2656, Accuracy: 0.8988\n  Batch 260/889 - Loss: 0.2656, Accuracy: 0.8986\n  Batch 270/889 - Loss: 0.2658, Accuracy: 0.8986\n  Batch 280/889 - Loss: 0.2655, Accuracy: 0.8990\n  Batch 290/889 - Loss: 0.2663, Accuracy: 0.8988\n  Batch 300/889 - Loss: 0.2655, Accuracy: 0.8991\n  Batch 310/889 - Loss: 0.2655, Accuracy: 0.8992\n  Batch 320/889 - Loss: 0.2643, Accuracy: 0.8997\n  Batch 330/889 - Loss: 0.2649, Accuracy: 0.8995\n  Batch 340/889 - Loss: 0.2641, Accuracy: 0.8998\n  Batch 350/889 - Loss: 0.2639, Accuracy: 0.9001\n  Batch 360/889 - Loss: 0.2637, Accuracy: 0.9003\n  Batch 370/889 - Loss: 0.2649, Accuracy: 0.9001\n  Batch 380/889 - Loss: 0.2671, Accuracy: 0.8992\n  Batch 390/889 - Loss: 0.2683, Accuracy: 0.8989\n  Batch 400/889 - Loss: 0.2693, Accuracy: 0.8984\n  Batch 410/889 - Loss: 0.2697, Accuracy: 0.8983\n  Batch 420/889 - Loss: 0.2692, Accuracy: 0.8984\n  Batch 430/889 - Loss: 0.2694, Accuracy: 0.8986\n  Batch 440/889 - Loss: 0.2698, Accuracy: 0.8986\n  Batch 450/889 - Loss: 0.2709, Accuracy: 0.8984\n  Batch 460/889 - Loss: 0.2705, Accuracy: 0.8981\n  Batch 470/889 - Loss: 0.2699, Accuracy: 0.8983\n  Batch 480/889 - Loss: 0.2700, Accuracy: 0.8985\n  Batch 490/889 - Loss: 0.2695, Accuracy: 0.8985\n  Batch 500/889 - Loss: 0.2696, Accuracy: 0.8985\n  Batch 510/889 - Loss: 0.2697, Accuracy: 0.8985\n  Batch 520/889 - Loss: 0.2693, Accuracy: 0.8984\n  Batch 530/889 - Loss: 0.2702, Accuracy: 0.8981\n  Batch 540/889 - Loss: 0.2695, Accuracy: 0.8983\n  Batch 550/889 - Loss: 0.2692, Accuracy: 0.8985\n  Batch 560/889 - Loss: 0.2687, Accuracy: 0.8987\n  Batch 570/889 - Loss: 0.2693, Accuracy: 0.8985\n  Batch 580/889 - Loss: 0.2694, Accuracy: 0.8985\n  Batch 590/889 - Loss: 0.2687, Accuracy: 0.8987\n  Batch 600/889 - Loss: 0.2681, Accuracy: 0.8988\n  Batch 610/889 - Loss: 0.2683, Accuracy: 0.8985\n  Batch 620/889 - Loss: 0.2679, Accuracy: 0.8988\n  Batch 630/889 - Loss: 0.2679, Accuracy: 0.8988\n  Batch 640/889 - Loss: 0.2691, Accuracy: 0.8985\n  Batch 650/889 - Loss: 0.2690, Accuracy: 0.8986\n  Batch 660/889 - Loss: 0.2694, Accuracy: 0.8987\n  Batch 670/889 - Loss: 0.2703, Accuracy: 0.8983\n  Batch 680/889 - Loss: 0.2705, Accuracy: 0.8982\n  Batch 690/889 - Loss: 0.2702, Accuracy: 0.8982\n  Batch 700/889 - Loss: 0.2698, Accuracy: 0.8980\n  Batch 710/889 - Loss: 0.2696, Accuracy: 0.8979\n  Batch 720/889 - Loss: 0.2694, Accuracy: 0.8980\n  Batch 730/889 - Loss: 0.2687, Accuracy: 0.8982\n  Batch 740/889 - Loss: 0.2685, Accuracy: 0.8983\n  Batch 750/889 - Loss: 0.2688, Accuracy: 0.8981\n  Batch 760/889 - Loss: 0.2685, Accuracy: 0.8981\n  Batch 770/889 - Loss: 0.2683, Accuracy: 0.8981\n  Batch 780/889 - Loss: 0.2683, Accuracy: 0.8979\n  Batch 790/889 - Loss: 0.2689, Accuracy: 0.8976\n  Batch 800/889 - Loss: 0.2691, Accuracy: 0.8976\n  Batch 810/889 - Loss: 0.2694, Accuracy: 0.8973\n  Batch 820/889 - Loss: 0.2691, Accuracy: 0.8974\n  Batch 830/889 - Loss: 0.2691, Accuracy: 0.8974\n  Batch 840/889 - Loss: 0.2691, Accuracy: 0.8974\n  Batch 850/889 - Loss: 0.2690, Accuracy: 0.8974\n  Batch 860/889 - Loss: 0.2695, Accuracy: 0.8971\n  Batch 870/889 - Loss: 0.2691, Accuracy: 0.8973\n  Batch 880/889 - Loss: 0.2689, Accuracy: 0.8974\n\nKết quả Epoch 8:\n  Loss trung bình: 0.2691\n  Độ chính xác trung bình: 0.8971\n  -> Cập nhật mô hình tốt nhất.\n\n===== Epoch 9 / 10 =====\n  Batch 10/889 - Loss: 0.2621, Accuracy: 0.9034\n  Batch 20/889 - Loss: 0.2702, Accuracy: 0.9033\n  Batch 30/889 - Loss: 0.2570, Accuracy: 0.9057\n  Batch 40/889 - Loss: 0.2585, Accuracy: 0.9040\n  Batch 50/889 - Loss: 0.2468, Accuracy: 0.9075\n  Batch 60/889 - Loss: 0.2508, Accuracy: 0.9034\n  Batch 70/889 - Loss: 0.2472, Accuracy: 0.9040\n  Batch 80/889 - Loss: 0.2447, Accuracy: 0.9041\n  Batch 90/889 - Loss: 0.2456, Accuracy: 0.9045\n  Batch 100/889 - Loss: 0.2441, Accuracy: 0.9058\n  Batch 110/889 - Loss: 0.2464, Accuracy: 0.9054\n  Batch 120/889 - Loss: 0.2458, Accuracy: 0.9059\n  Batch 130/889 - Loss: 0.2466, Accuracy: 0.9054\n  Batch 140/889 - Loss: 0.2512, Accuracy: 0.9034\n  Batch 150/889 - Loss: 0.2515, Accuracy: 0.9035\n  Batch 160/889 - Loss: 0.2538, Accuracy: 0.9016\n  Batch 170/889 - Loss: 0.2537, Accuracy: 0.9020\n  Batch 180/889 - Loss: 0.2519, Accuracy: 0.9024\n  Batch 190/889 - Loss: 0.2530, Accuracy: 0.9021\n  Batch 200/889 - Loss: 0.2524, Accuracy: 0.9024\n  Batch 210/889 - Loss: 0.2529, Accuracy: 0.9023\n  Batch 220/889 - Loss: 0.2533, Accuracy: 0.9025\n  Batch 230/889 - Loss: 0.2533, Accuracy: 0.9026\n  Batch 240/889 - Loss: 0.2531, Accuracy: 0.9029\n  Batch 250/889 - Loss: 0.2531, Accuracy: 0.9033\n  Batch 260/889 - Loss: 0.2525, Accuracy: 0.9035\n  Batch 270/889 - Loss: 0.2536, Accuracy: 0.9030\n  Batch 280/889 - Loss: 0.2528, Accuracy: 0.9031\n  Batch 290/889 - Loss: 0.2519, Accuracy: 0.9038\n  Batch 300/889 - Loss: 0.2525, Accuracy: 0.9037\n  Batch 310/889 - Loss: 0.2531, Accuracy: 0.9037\n  Batch 320/889 - Loss: 0.2549, Accuracy: 0.9033\n  Batch 330/889 - Loss: 0.2541, Accuracy: 0.9041\n  Batch 340/889 - Loss: 0.2548, Accuracy: 0.9037\n  Batch 350/889 - Loss: 0.2559, Accuracy: 0.9036\n  Batch 360/889 - Loss: 0.2550, Accuracy: 0.9038\n  Batch 370/889 - Loss: 0.2540, Accuracy: 0.9044\n  Batch 380/889 - Loss: 0.2556, Accuracy: 0.9039\n  Batch 390/889 - Loss: 0.2569, Accuracy: 0.9032\n  Batch 400/889 - Loss: 0.2575, Accuracy: 0.9030\n  Batch 410/889 - Loss: 0.2578, Accuracy: 0.9029\n  Batch 420/889 - Loss: 0.2572, Accuracy: 0.9032\n  Batch 430/889 - Loss: 0.2571, Accuracy: 0.9032\n  Batch 440/889 - Loss: 0.2566, Accuracy: 0.9032\n  Batch 450/889 - Loss: 0.2571, Accuracy: 0.9029\n  Batch 460/889 - Loss: 0.2576, Accuracy: 0.9026\n  Batch 470/889 - Loss: 0.2569, Accuracy: 0.9032\n  Batch 480/889 - Loss: 0.2574, Accuracy: 0.9032\n  Batch 490/889 - Loss: 0.2570, Accuracy: 0.9033\n  Batch 500/889 - Loss: 0.2567, Accuracy: 0.9035\n  Batch 510/889 - Loss: 0.2571, Accuracy: 0.9033\n  Batch 520/889 - Loss: 0.2576, Accuracy: 0.9030\n  Batch 530/889 - Loss: 0.2579, Accuracy: 0.9031\n  Batch 540/889 - Loss: 0.2575, Accuracy: 0.9032\n  Batch 550/889 - Loss: 0.2570, Accuracy: 0.9035\n  Batch 560/889 - Loss: 0.2569, Accuracy: 0.9035\n  Batch 570/889 - Loss: 0.2565, Accuracy: 0.9038\n  Batch 580/889 - Loss: 0.2567, Accuracy: 0.9037\n  Batch 590/889 - Loss: 0.2562, Accuracy: 0.9038\n  Batch 600/889 - Loss: 0.2558, Accuracy: 0.9038\n  Batch 610/889 - Loss: 0.2557, Accuracy: 0.9037\n  Batch 620/889 - Loss: 0.2563, Accuracy: 0.9034\n  Batch 630/889 - Loss: 0.2560, Accuracy: 0.9035\n  Batch 640/889 - Loss: 0.2568, Accuracy: 0.9033\n  Batch 650/889 - Loss: 0.2566, Accuracy: 0.9034\n  Batch 660/889 - Loss: 0.2560, Accuracy: 0.9038\n  Batch 670/889 - Loss: 0.2561, Accuracy: 0.9038\n  Batch 680/889 - Loss: 0.2562, Accuracy: 0.9036\n  Batch 690/889 - Loss: 0.2563, Accuracy: 0.9036\n  Batch 700/889 - Loss: 0.2557, Accuracy: 0.9038\n  Batch 710/889 - Loss: 0.2561, Accuracy: 0.9037\n  Batch 720/889 - Loss: 0.2556, Accuracy: 0.9040\n  Batch 730/889 - Loss: 0.2556, Accuracy: 0.9041\n  Batch 740/889 - Loss: 0.2558, Accuracy: 0.9040\n  Batch 750/889 - Loss: 0.2561, Accuracy: 0.9039\n  Batch 760/889 - Loss: 0.2559, Accuracy: 0.9040\n  Batch 770/889 - Loss: 0.2558, Accuracy: 0.9040\n  Batch 780/889 - Loss: 0.2555, Accuracy: 0.9041\n  Batch 790/889 - Loss: 0.2554, Accuracy: 0.9041\n  Batch 800/889 - Loss: 0.2554, Accuracy: 0.9040\n  Batch 810/889 - Loss: 0.2551, Accuracy: 0.9042\n  Batch 820/889 - Loss: 0.2551, Accuracy: 0.9044\n  Batch 830/889 - Loss: 0.2551, Accuracy: 0.9043\n  Batch 840/889 - Loss: 0.2549, Accuracy: 0.9044\n  Batch 850/889 - Loss: 0.2547, Accuracy: 0.9045\n  Batch 860/889 - Loss: 0.2547, Accuracy: 0.9045\n  Batch 870/889 - Loss: 0.2545, Accuracy: 0.9045\n  Batch 880/889 - Loss: 0.2547, Accuracy: 0.9045\n\nKết quả Epoch 9:\n  Loss trung bình: 0.2549\n  Độ chính xác trung bình: 0.9045\n  -> Cập nhật mô hình tốt nhất.\n\n===== Epoch 10 / 10 =====\n  Batch 10/889 - Loss: 0.2233, Accuracy: 0.9205\n  Batch 20/889 - Loss: 0.2688, Accuracy: 0.9048\n  Batch 30/889 - Loss: 0.2553, Accuracy: 0.9088\n  Batch 40/889 - Loss: 0.2610, Accuracy: 0.9051\n  Batch 50/889 - Loss: 0.2596, Accuracy: 0.9062\n  Batch 60/889 - Loss: 0.2515, Accuracy: 0.9086\n  Batch 70/889 - Loss: 0.2486, Accuracy: 0.9085\n  Batch 80/889 - Loss: 0.2472, Accuracy: 0.9080\n  Batch 90/889 - Loss: 0.2472, Accuracy: 0.9085\n  Batch 100/889 - Loss: 0.2442, Accuracy: 0.9092\n  Batch 110/889 - Loss: 0.2420, Accuracy: 0.9106\n  Batch 120/889 - Loss: 0.2405, Accuracy: 0.9115\n  Batch 130/889 - Loss: 0.2406, Accuracy: 0.9116\n  Batch 140/889 - Loss: 0.2395, Accuracy: 0.9117\n  Batch 150/889 - Loss: 0.2385, Accuracy: 0.9123\n  Batch 160/889 - Loss: 0.2377, Accuracy: 0.9125\n  Batch 170/889 - Loss: 0.2366, Accuracy: 0.9128\n  Batch 180/889 - Loss: 0.2368, Accuracy: 0.9129\n  Batch 190/889 - Loss: 0.2371, Accuracy: 0.9127\n  Batch 200/889 - Loss: 0.2373, Accuracy: 0.9125\n  Batch 210/889 - Loss: 0.2409, Accuracy: 0.9114\n  Batch 220/889 - Loss: 0.2405, Accuracy: 0.9113\n  Batch 230/889 - Loss: 0.2411, Accuracy: 0.9110\n  Batch 240/889 - Loss: 0.2434, Accuracy: 0.9099\n  Batch 250/889 - Loss: 0.2427, Accuracy: 0.9108\n  Batch 260/889 - Loss: 0.2434, Accuracy: 0.9104\n  Batch 270/889 - Loss: 0.2435, Accuracy: 0.9099\n  Batch 280/889 - Loss: 0.2443, Accuracy: 0.9095\n  Batch 290/889 - Loss: 0.2446, Accuracy: 0.9095\n  Batch 300/889 - Loss: 0.2446, Accuracy: 0.9093\n  Batch 310/889 - Loss: 0.2435, Accuracy: 0.9096\n  Batch 320/889 - Loss: 0.2428, Accuracy: 0.9095\n  Batch 330/889 - Loss: 0.2430, Accuracy: 0.9092\n  Batch 340/889 - Loss: 0.2438, Accuracy: 0.9091\n  Batch 350/889 - Loss: 0.2434, Accuracy: 0.9096\n  Batch 360/889 - Loss: 0.2443, Accuracy: 0.9093\n  Batch 370/889 - Loss: 0.2432, Accuracy: 0.9096\n  Batch 380/889 - Loss: 0.2422, Accuracy: 0.9099\n  Batch 390/889 - Loss: 0.2428, Accuracy: 0.9098\n  Batch 400/889 - Loss: 0.2431, Accuracy: 0.9096\n  Batch 410/889 - Loss: 0.2425, Accuracy: 0.9099\n  Batch 420/889 - Loss: 0.2433, Accuracy: 0.9094\n  Batch 430/889 - Loss: 0.2434, Accuracy: 0.9093\n  Batch 440/889 - Loss: 0.2444, Accuracy: 0.9091\n  Batch 450/889 - Loss: 0.2445, Accuracy: 0.9092\n  Batch 460/889 - Loss: 0.2449, Accuracy: 0.9090\n  Batch 470/889 - Loss: 0.2452, Accuracy: 0.9088\n  Batch 480/889 - Loss: 0.2449, Accuracy: 0.9088\n  Batch 490/889 - Loss: 0.2444, Accuracy: 0.9090\n  Batch 500/889 - Loss: 0.2454, Accuracy: 0.9084\n  Batch 510/889 - Loss: 0.2451, Accuracy: 0.9085\n  Batch 520/889 - Loss: 0.2450, Accuracy: 0.9083\n  Batch 530/889 - Loss: 0.2443, Accuracy: 0.9086\n  Batch 540/889 - Loss: 0.2440, Accuracy: 0.9087\n  Batch 550/889 - Loss: 0.2439, Accuracy: 0.9089\n  Batch 560/889 - Loss: 0.2443, Accuracy: 0.9088\n  Batch 570/889 - Loss: 0.2436, Accuracy: 0.9090\n  Batch 580/889 - Loss: 0.2435, Accuracy: 0.9088\n  Batch 590/889 - Loss: 0.2433, Accuracy: 0.9090\n  Batch 600/889 - Loss: 0.2428, Accuracy: 0.9093\n  Batch 610/889 - Loss: 0.2428, Accuracy: 0.9094\n  Batch 620/889 - Loss: 0.2422, Accuracy: 0.9098\n  Batch 630/889 - Loss: 0.2416, Accuracy: 0.9100\n  Batch 640/889 - Loss: 0.2421, Accuracy: 0.9098\n  Batch 650/889 - Loss: 0.2423, Accuracy: 0.9099\n  Batch 660/889 - Loss: 0.2432, Accuracy: 0.9093\n  Batch 670/889 - Loss: 0.2433, Accuracy: 0.9093\n  Batch 680/889 - Loss: 0.2435, Accuracy: 0.9093\n  Batch 690/889 - Loss: 0.2434, Accuracy: 0.9093\n  Batch 700/889 - Loss: 0.2434, Accuracy: 0.9093\n  Batch 710/889 - Loss: 0.2430, Accuracy: 0.9096\n  Batch 720/889 - Loss: 0.2431, Accuracy: 0.9096\n  Batch 730/889 - Loss: 0.2428, Accuracy: 0.9098\n  Batch 740/889 - Loss: 0.2426, Accuracy: 0.9097\n  Batch 750/889 - Loss: 0.2430, Accuracy: 0.9097\n  Batch 760/889 - Loss: 0.2423, Accuracy: 0.9099\n  Batch 770/889 - Loss: 0.2427, Accuracy: 0.9099\n  Batch 780/889 - Loss: 0.2431, Accuracy: 0.9096\n  Batch 790/889 - Loss: 0.2433, Accuracy: 0.9094\n  Batch 800/889 - Loss: 0.2426, Accuracy: 0.9096\n  Batch 810/889 - Loss: 0.2431, Accuracy: 0.9094\n  Batch 820/889 - Loss: 0.2436, Accuracy: 0.9091\n  Batch 830/889 - Loss: 0.2436, Accuracy: 0.9092\n  Batch 840/889 - Loss: 0.2439, Accuracy: 0.9091\n  Batch 850/889 - Loss: 0.2443, Accuracy: 0.9091\n  Batch 860/889 - Loss: 0.2440, Accuracy: 0.9093\n  Batch 870/889 - Loss: 0.2438, Accuracy: 0.9093\n  Batch 880/889 - Loss: 0.2436, Accuracy: 0.9093\n\nKết quả Epoch 10:\n  Loss trung bình: 0.2438\n  Độ chính xác trung bình: 0.9092\n  -> Cập nhật mô hình tốt nhất.\nHoàn tất huấn luyện.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m save_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_phobert_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Tạo thư mục lưu nếu chưa tồn tại\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_directory):\n\u001b[1;32m    131\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_directory)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Lưu mô hình\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"],"ename":"NameError","evalue":"name 'os' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Lưu mô hình và tokenizer sử dụng save_pretrained\nsave_directory = 'saved_phobert_model'\n\n# Tạo thư mục lưu nếu chưa tồn tại\nif not os.path.exists(save_directory):\n    os.makedirs(save_directory)\n\n# Lưu mô hình\nmodel.save_pretrained(save_directory)\n\n# Lưu tokenizer\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"Mô hình và tokenizer đã được lưu tại {save_directory}\")\n\n# Đánh giá mô hình trên tập kiểm tra\nprint(\"\\nBắt đầu đánh giá mô hình trên tập kiểm tra...\")\nmodel.load_state_dict(torch.load('/kaggle/working/best_model.pt'))  # Tải mô hình tốt nhất\nmodel.eval()\ny_pred = []\ny_true = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        y_pred.extend(predictions.cpu().numpy())\n        y_true.extend(labels.cpu().numpy())\n\nprint(\"Hoàn tất đánh giá.\")\n\nprint(\"\\nBáo cáo đánh giá:\")\nprint(classification_report(y_true, y_pred, digits=5))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T07:29:06.599268Z","iopub.execute_input":"2024-10-17T07:29:06.599676Z","iopub.status.idle":"2024-10-17T07:30:49.219479Z","shell.execute_reply.started":"2024-10-17T07:29:06.599636Z","shell.execute_reply":"2024-10-17T07:30:49.218461Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Mô hình và tokenizer đã được lưu tại saved_phobert_model\n\nBắt đầu đánh giá mô hình trên tập kiểm tra...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2468469313.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/best_model.pt'))  # Tải mô hình tốt nhất\n","output_type":"stream"},{"name":"stdout","text":"Hoàn tất đánh giá.\n\nBáo cáo đánh giá:\n              precision    recall  f1-score   support\n\n           0    0.85888   0.83109   0.84476      2380\n           1    0.79031   0.83844   0.81367      2315\n           2    0.79199   0.76149   0.77644      2415\n           3    0.93977   0.96233   0.95092      2416\n           4    0.90081   0.88923   0.89498      2257\n           5    0.98444   0.98444   0.98444      2442\n\n    accuracy                        0.87831     14225\n   macro avg    0.87770   0.87784   0.87753     14225\nweighted avg    0.87831   0.87831   0.87808     14225\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# Đường dẫn tới thư mục đã lưu mô hình\nsave_directory = '/kaggle/working/saved_phobert_model'\n\n# Tải tokenizer\ntokenizer = AutoTokenizer.from_pretrained(save_directory)\n\n# Tải mô hình\nmodel = AutoModelForSequenceClassification.from_pretrained(save_directory, num_labels=6)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nprint(\"Mô hình và tokenizer đã được tải lại thành công.\")\n\n# Hàm dự đoán\ndef predict(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predicted_class = torch.argmax(logits, dim=-1).item()\n    return predicted_class\n\n# Ví dụ dự đoán\nsample_comment = \"hàng không sử dụng được, tốn tiền mua nhưng về vứt xó không sử dụng\"\npredicted_label = predict(sample_comment)\nprint(f\"Bình luận: '{sample_comment}' được phân loại vào lớp: {predicted_label}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T07:37:14.061799Z","iopub.execute_input":"2024-10-17T07:37:14.062171Z","iopub.status.idle":"2024-10-17T07:37:14.511917Z","shell.execute_reply.started":"2024-10-17T07:37:14.062134Z","shell.execute_reply":"2024-10-17T07:37:14.510983Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Mô hình và tokenizer đã được tải lại thành công.\nBình luận: 'hàng không sử dụng được, tốn tiền mua nhưng về vứt xó không sử dụng' được phân loại vào lớp: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}